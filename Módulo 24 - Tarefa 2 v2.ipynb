{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce132bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGANDO BIBLIOTECAS NECESSÁRIAS\n",
    "\n",
    "import numpy as np  # Biblioteca para operações numéricas eficientes e arrays multidimensionais.\n",
    "from sklearn.metrics import mean_squared_error  # Métrica para calcular o erro médio quadrático.\n",
    "from sklearn.datasets import make_friedman1  # Função para gerar o conjunto de dados sintético Friedman 1.\n",
    "from sklearn.datasets import make_hastie_10_2  # Função para gerar o conjunto de dados sintético Hastie 10-2.\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Algoritmo de boosting para regressão.\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # Algoritmo de boosting para classificação.\n",
    "from sklearn.model_selection import GridSearchCV  # Ferramenta para busca em grade e validação cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28cd0cd",
   "metadata": {},
   "source": [
    "<font size=\"4\">**TAREFA 2**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67668a",
   "metadata": {},
   "source": [
    "<font size=\"3\">**1. Cite 5 diferenças entre o AdaBoost e o GBM.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f0698",
   "metadata": {},
   "source": [
    "> 1 - **ABORDAGEM**\n",
    "\n",
    "<font size=\"2\"> No **AdaBoost** pondera os erros anteriores para focar nos exemplos mais complexos, já no **GBM** os erros anteriores são corrigidos ao ajustar os modelos seguintes. </font>\n",
    "\n",
    "> 2 - **MODELOS BASE**\n",
    "\n",
    "<font size=\"2\"> No **AdaBoost** são utilizados modelos simples, como as árvores de decisão com apenas um nó, já no **GBM** são utilizados modelos mais complexos, geralmente árvores de decisão completas. </font>\n",
    "\n",
    "> 3 - **TREINAMENTO**\n",
    "\n",
    "<font size=\"2\"> No **AdaBoost** são atribuídos pesos aos exemplos e iteração para melhorar a classificação dos mal classificados, já no **GBM** os modelos sucessicivos são ajustados para os resíduos (diferenças) do modelo anterior. </font>\n",
    "\n",
    "> 4 - **PARALELISMO**\n",
    "\n",
    "<font size=\"2\"> No **AdaBoost** os modelos são treinados sequencialmente, um após o outro, já no **GBM** o paralelismo pode ser explorado, treinando árvores independentemente. </font>\n",
    "\n",
    "> 5 - **SENSIBILIDADE A OUTLIERS**\n",
    "\n",
    "<font size=\"2\"> No **AdaBoost** há uma maior sensibilidade, pois o método foca em exemplos difíceis e pode ser afetado por outliers, já no **GBM** devido ao ajuste gradativo e à capacidade das árvores de lidar com outliers, o método é menos sensível. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef5d01",
   "metadata": {},
   "source": [
    "<font size=\"3\">**2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a69868",
   "metadata": {},
   "source": [
    "<font size=\"4\"><u>CLASSIFICAÇÃO </u></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d2ea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.913\n"
     ]
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "formatted_score = round(score, 3)\n",
    "\n",
    "print(\"Score:\", formatted_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e511434",
   "metadata": {},
   "source": [
    "<font size=\"4\"><b>4. UTILIZANDO GRIDSEARCH PARA ENCONTRAR MELHORES PARÂMETROS DO EXEMPLO ACIMA</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4603fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 200}\n",
      "Best Score: 0.911\n"
     ]
    }
   ],
   "source": [
    "# Carregando o conjunto de dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definindo os hiperparâmetros que deseja testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Criando o classificador\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Criando o GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo o melhor classificador a partir do GridSearchCV\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Calculando e formatando a pontuação do melhor classificador\n",
    "score = best_clf.score(X_test, y_test)\n",
    "formatted_score = round(score, 3)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", formatted_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd4300",
   "metadata": {},
   "source": [
    "<font size=\"4\"><u>REGRESSÃO</u></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bba91522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.009\n"
     ]
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error').fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, est.predict(X_test))\n",
    "formatted_mse = round(mse, 3)\n",
    "\n",
    "print(\"Mean Squared Error:\", formatted_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae57fb",
   "metadata": {},
   "source": [
    "<font size=\"4\"><b>4. UTILIZANDO GRIDSEARCH PARA ENCONTRAR MELHORES PARÂMETROS DO EXEMPLO ACIMA</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03092aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 200}\n",
      "Best Mean Squared Error: 3.654\n"
     ]
    }
   ],
   "source": [
    "# Gerando o conjunto de dados\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Definindo os hiperparâmetros que deseja testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Criando o regressor\n",
    "est = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "# Criando o GridSearchCV\n",
    "grid_search = GridSearchCV(est, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo o melhor regressor a partir do GridSearchCV\n",
    "best_est = grid_search.best_estimator_\n",
    "\n",
    "# Calculando e formatando o Mean Squared Error do melhor regressor\n",
    "mse = mean_squared_error(y_test, best_est.predict(X_test))\n",
    "formatted_mse = round(mse, 3)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean Squared Error:\", formatted_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b7480",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3. Cite 5 Hyperparametros importantes no GBM.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d59e7f",
   "metadata": {},
   "source": [
    "> 1 - **n_estimators**\n",
    "\n",
    "<font size=\"2\"> Número de árvores no ensemble (combinação de modelos para melhoria). </font>\n",
    "\n",
    "> 2 - **learning_rate**\n",
    "\n",
    "<font size=\"2\"> Taxa de aprendizado para ajustar as previsões. </font>\n",
    "\n",
    "> 3 - **max_depth**\n",
    "\n",
    "<font size=\"2\"> Profundidade máxima das árvores. </font>\n",
    "\n",
    "> 4 - **subsample**\n",
    "\n",
    "<font size=\"2\"> Fração de amostrass usadas para treinar cada árvore. </font>\n",
    "\n",
    "> 5 - **loss**\n",
    "\n",
    "<font size=\"2\"> Função de perde a ser otimizada durante o treinamento.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8361650",
   "metadata": {},
   "source": [
    "<font size=\"3\">**5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7406bb",
   "metadata": {},
   "source": [
    "> <font size=\"2\"> A maior diferença entre os dois algoritmos, Gradient Boosting e Stochastic Gradient Boosting, é que no primeiro são ajustados sequencialmente os modelos fracos para corrigir os erros cometidos pelos modelos anteriores, snedo que a cada novo modelo é ajustado aos resíduos entre as previsões do modelo anterior e os valores reais, com foco em melhorar os erros persistentes e tornando-o mais preciso à medida que mais modelos são adicionados, sendo de extrema importância que o modelo seja cuidadosamente ajustado para evitar o overfitting, já no segundo algoritmo é introduzido um componente de aleatoriedade durante o treinamento, ou seja, ao inves de ajustar a cada iteração o modelo a todos os dados de treinamento, ele seleciona aleatoriamente uma subamostra (amostra parcial de dados) para treinamento a cada iteração (passo repetido no processo), isso introduz uma variação adicional ao processo de ajuste, tornando  o modelo mais robusto (resistente a variações) e reduzindo a sensibilidade ao overfitting (superajuste aos dados). </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
